# -*- coding: utf-8 -*-
"""Lab_3_Denis_Sizov.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q3Ap1m94afpnlO-tEWIkjs510KUeGeid

# <center> Майнор "Интеллектуальный анализ данных" </center>

# <center> Курс "Прикладные задачи анализа данных" </center>

# <center>Лабораторная работа №1. NLP</center>

## Задание

В рамках данной лабораторной работы вам предлагается решить задачу регрессии работая с текстами: предсказать уровень заработной платы по используя текст.

Подробнее о данных можно почитать на [kaggle](https://www.kaggle.com/c/job-salary-prediction/data)
Данные можно скачать там же (Train_rev1.*) или по ссылке на яндекс диске [yadisk_url](https://yadi.sk/d/vVEOWPFY3NruT7) 

Баллы за лабораторную будут выставляться по следующим блокам:


*   Text preprocessing - 2 балла
*   Text representation (bag of words, words frequency, tf-idf, LSA, W2V, Fasttext... Honor track: BERT, Elmo) - 3 балла
*   Classical ML algorithms (Linear regression, SVM, ensembles of trees etc.) - 2 балла 
*   Neural networks: RNN (LSTM, GRU) + attention, Transformer - 3 балла

Я рекомендую обратить внимания на kernel'ы соревнования, а так же на следующие ноутбуки: [этот](https://github.com/yandexdataschool/nlp_course/blob/2019/week02_classification/seminar.ipynb) и [этот](https://github.com/yandexdataschool/nlp_course/blob/2019/week02_classification/homework_part2.ipynb)

Кто прикрутит сверточные нейросетки для этой задачи - получит +1 бонусный бал.

**Дедлайн - 22 марта 23:59**

Задача лабораторной - научиться работать с текстом. Постарайтесь подойти к выполнениею не формально, а так чтобы Вам самим было интересно и понравился результат )
Exploritary data analysis (картинки и графики) и Вашы выводы приветствуются. Для понижения размерности помимо прочих алгоритмов (PCA, SVD, TSNE etc.) попробуйте UMAP.

Постарайтесь прокомментировать каждый блок пайплайна, сравнить различные алгоритмы, сделать выводы.
"""

from sklearn.pipeline import *
from sklearn.preprocessing import *
from sklearn.decomposition import *
from sklearn.linear_model import *
from sklearn.ensemble import *
from sklearn.compose import *
from sklearn.feature_extraction.text import *
from sklearn.model_selection import *
import numpy as np
import pandas as pd

# from google.colab import files
# uploaded = files.upload()

!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
# Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

link = 'https://drive.google.com/open?id=17br_4807K9bpjxjWLHK4aR00pE1qtabC'
fluff, id = link.split('=')

downloaded = drive.CreateFile({'id':id}) 
downloaded.GetContentFile('Train_rev1.csv')  
df = pd.read_csv('Train_rev1.csv')

df.head()

"""## Преобработка

Сразу удалим столбцы: LocationRaw, SalaryRaw, Id, SourceName. Первые 2 сырые, вторые 2 бессмысленные
"""

data = df.drop(df.columns[[0, 3, 9, 11]], axis='columns')

data.head(5)

data['Title'] = data['Title'].astype(str)
data['FullDescription'] = data['FullDescription'].astype(str)

text_columns = ["Title", "FullDescription"]
categorical_columns = ["Category", "Company", "LocationNormalized", "ContractType", "ContractTime"]
target_column = "SalaryNormalized"

"""Заменим все пропуски на NaN"""

data[categorical_columns] = data[categorical_columns].fillna('NaN')

data['Title'] = data['Title'].str.lower()
data['FullDescription'] = data['FullDescription'].str.lower()

import nltk
import string
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('wordnet')
nltk.download('punkt')
nltk.download('stopwords')
lemmatizer = WordNetLemmatizer()
stop_words = stopwords.words('english')

"""Сделаем токенизацию, уберем все символы и цифры, удалим все английские стоп-слова"""

def tokenize_me(file_text):
  stop_symbol = string.punctuation + string.digits
  line_clear = [i for i in file_text if (i not in stop_symbol)]
  file_text = ""
  for i in range(len(line_clear)):
      file_text += line_clear[i]
  tokens = nltk.word_tokenize(file_text)
  tokens = [lemmatizer.lemmatize(i) for i in tokens if (i not in stop_words)]
  return tokens

"""Дальше будет видно, что эта строчка добавлена не сразу, а перейдя к более сложным методам анализа, так как время стало подходит к часам, поэтому в некоторых местах выведены данные для полного датасета, в некоторых только для четверти"""

# data = data.head(int(data.shape[0]/4))

data_title = data['Title'].copy()
data_description = data['FullDescription'].copy()
for i in range(len(data)):
  data_title[i] = tokenize_me(data['Title'][i])
  data_description[i] = tokenize_me(data['FullDescription'][i])

"""Заменим столбцы с заголовком и описанием на готовые массивы"""

data['Title'] = data_title
data['FullDescription'] = data_description

data

columns = ['Title',	'FullDescription',	'LocationNormalized',	'ContractType', 'ContractTime',	'Company',	'Category', 'SalaryNormalized']

"""Смотрим в каких местах больше всего NaN"""

for i in range(2, len(columns)-1):
  lists = data[columns[i]]
  counts = lists.value_counts().to_dict()
  if counts.get('NaN') != None:
    print(f"Percent of NaN in {columns[i]} = {round(counts.get('NaN')/len(data[columns[i]])*100, 2)}% in {len(counts)} types")

"""Удаляем столбец ContractType, так как там больше 50% пустых данных"""

data = data.drop('ContractType', axis='columns')

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
changed_columns = ['LocationNormalized',	'Company',	'Category']

"""Изменяем остальные столбцы на числовые данные. Столбец ContractTime делим на 2 бинарных permanent и concract."""

for i in range(len(changed_columns)):
    le.fit(data[changed_columns[i]])
    data[changed_columns[i]] = le.transform(data[changed_columns[i]])

from sklearn.preprocessing import OneHotEncoder
enc = OneHotEncoder(handle_unknown='ignore')
enc_df = pd.DataFrame(enc.fit_transform(data[['ContractTime']]).toarray())

enc_df = enc_df.drop(0, axis='columns')
enc_df.columns = ['ContractTime_contract','ContractTime_permanent']

data = data.join(enc_df)

data = data.drop('ContractTime', axis='columns')

data

"""Записываем в новый файл. Так как все постоянно вылетает"""

data.to_csv("new_df.csv")

import pandas as pd
data = pd.read_csv("new_df.csv")

"""## Методы текстового представления

Создаем строковый вариант, для анализа далее
"""

list_string_title = []
list_string_description = []
for i in range(len(data['Title'])):
  title_string = ' '.join(data['Title'][i])
  description_string = ' '.join(data['FullDescription'][i])
  list_string_title.append(title_string)
  list_string_description.append(description_string)

list_string = list_string_description + list_string_title

"""Используем CountVectorizer строим мешок"""

from sklearn.feature_extraction.text import CountVectorizer
vector_cv = CountVectorizer()
vector_cv.fit(list_string)
# print(vector_cv.vocabulary_)
vector = vector_cv.transform(["engineering analyst"])
print(vector.toarray())

"""И трансформируем каждую строчку в массив"""

cv_title = vector_cv.transform(list_string_title)
cv_description = vector_cv.transform(list_string_description)

"""Пробуем изменить размерность с помощью LDA

from sklearn.decomposition import LatentDirichletAllocation as LDA
lda = LDA(n_components = 5, max_iter=5)
lda.fit_transform(cv_title)

Плохая затея.

Дальше на очереди TfidfVectorizer
"""

from sklearn.feature_extraction.text import TfidfVectorizer
vector_tfv = TfidfVectorizer()
vector_tfv.fit(list_string)
# print(vector_tfv.vocabulary_)
print(vector_tfv.idf_)
vector = vector_tfv.transform(["engineering analyst"])
print(vector.toarray())

tfv_title = vector_tfv.transform(list_string_title)
tfv_description = vector_tfv.transform(list_string_description)

"""Пробуем Tokenizer из keras"""

from keras.preprocessing.text import Tokenizer
vector_tz = Tokenizer()
vector_tz.fit_on_texts(list_string)
print(f'Vocabulary: {list(vector_tz.word_index.keys())}')
vectors = vector_tz.texts_to_matrix(["engineering analyst"], mode='count')
print(vectors)

"""tfv_title = vector_tz.texts_to_matrix(list_string_title, mode='count')
tfv_description = vector_tz.texts_to_matrix(list_string_description, mode='count')

После первого прогона, вылетела вся программа. Не имея желания снова терять все и ждать заново, отказываемся от этой идеи

## Стандартные методы анализа
"""

from sklearn.feature_extraction import DictVectorizer
enc = DictVectorizer()
categories = enc.fit_transform(data[['LocationNormalized', 'Company', 'ContractTime_contract', 'ContractTime_permanent', 'Category']].to_dict('records'))

"""Делаем выборки для дальнейшего анализа"""

from scipy.sparse import hstack
result_cv = hstack([cv_title, cv_description, categories])
result_tfv = hstack([tfv_title, tfv_description, categories])

x_train_cv, x_test_cv, y_train_cv, y_test_cv = train_test_split(
    result_cv, 
    data["SalaryNormalized"],
    test_size=0.33, 
    random_state=1
)

x_train_tfv, x_test_tfv, y_train_tfv, y_test_tfv = train_test_split(
    result_tfv, 
    data["SalaryNormalized"],
    test_size=0.33, 
    random_state=1
)

from sklearn import linear_model
from sklearn import metrics
import matplotlib.pyplot as plt
import time

def metric(y_test, y_pred):
    plt.plot(y_pred[1:100], 'r-', label='Predicted')
    plt.plot(y_test[1:100].values, 'b-', label='Correct')
    plt.xlabel("Observations <Linear> ")
    plt.ylabel("Marks")
    plt.legend(loc='best')
    plt.show()
 
    MSE = metrics.mean_squared_error(y_pred=y_test, y_true=y_pred) 
    RMSE = np.sqrt(MSE)
    MAE = metrics.mean_absolute_error(y_pred=y_test, y_true=y_pred)
    MEDIAN = metrics.median_absolute_error(y_pred=y_test, y_true=y_pred)
    R2 = metrics.r2_score(y_pred=y_test, y_true=y_pred) 
    print("Metrics:")
    print("MSE:   {}\nRMSE:  {}\nR2:    {}\nMAE:   {}\nMedae: {}".format(MSE, RMSE, R2, MAE, MEDIAN))

linear = linear_model.LinearRegression()
linear.fit(x_train_cv, y_train_cv)

y_pred = linear.predict(x_test_cv)
metric(y_test_cv, y_pred)

"""CountVectorizer c LinearRegression дают R2 = 0.51. Но по времени это считало очень долго
При четверти датасета выдает 0.29
"""

t = time.time()
linear = linear_model.LinearRegression()
linear.fit(x_train_tfv, y_train_tfv)

y_pred = linear.predict(x_test_tfv)
metric(y_test_tfv, y_pred)
print(time.time()-t)

"""tf-idf c LinearRegression дают R2 = 0.56 и причем выдает результат очень быстро
И 0.4 при четверти всего датасета

В связи с такой разницей дальше будем анализировать только tf-idf

Пробуем построить модель Word2Vec
"""

pip install --upgrade gensim

from gensim.models import Word2Vec

model_wc = Word2Vec(list_string, min_count=1, size=100, window=5, sg=1)

len(model_wc.wv.vocab)

"""Получилось, вот только в этом нет смысла, так как он определяет похожесть слов. Аналогично с Honor track: BERT, Elmo, очень мощные инструменты, но в данной работе нам не подходят"""

pip install fasttext

full_string = ''
for i in range(len(list_string)):
  full_string += list_string[i]

file1 = open("text.txt","w+") 
file1.write(full_string)

"""Поработаем с fasttext. Для начала надо подготовить данные"""

import fasttext

model_fast = fasttext.train_unsupervised("text.txt", model='skipgram')

print(model_fast.words)   
print(model_fast['engineering analyst'])

print(list_string_title[3])
print(list_string_description[6])

vector_fast_title = []
vecror_fast_description = []
for i in range(len(list_string_title)):
  vector_fast_title.append(model_fast[list_string_title[i]])
for i in range(len(list_string_description)):
  vecror_fast_description.append(model_fast[list_string_description[i]])

result_fast = hstack([vector_fast_title, vecror_fast_description, categories])

x_train_fast, x_test_fast, y_train_fast, y_test_fast = train_test_split(
    result_fast, 
    data["SalaryNormalized"],
    test_size=0.33, 
    random_state=1
)

t = time.time()
linear = linear_model.LinearRegression()
linear.fit(x_train_fast, y_train_fast)

y_pred = linear.predict(x_test_fast)
metric(y_test_fast, y_pred)
print(time.time()-t)

"""Конечно показатель R2 не очень, но при этом RMSE даже лучше чем было. По графику видно что отклонения не совсем большие. И учитывая что он считал всего 5 секунд, это отличный результат. Метод оправдывает свое название

Посчитал для всего датасета. Получились вот такие результаты: 
R2:    -0.031244348111507447
RMSE: 12618.69947878338
Результаты особо не отличается при маленькой области обучения

Дальше будем считать все на методе TfidfVectorizer, так как в целом он показывает лучшие результаты
"""

from sklearn.svm import SVR

clf = SVR(C=1.0, epsilon=0.2)
clf.fit(x_train_tfv, y_train_tfv)

y_pred = clf.predict(x_test_tfv)
metric(y_test_tfv, y_pred)

"""Спустя 20 минут было принято решение выключить задачу, так как она работала уже слишком долгое время"""

lassoModel = linear_model.LassoCV()
lassoModel.fit(x_train_tfv, y_train_tfv)    
prediction = lassoModel.predict(x_test_tfv)

lasso_metrics = printMetrics(y_test_tfv, prediction)

"""Лассо пришлось ждать час, без результатов, не известно сколько еще это могло идти"""

from sklearn.ensemble import RandomForestRegressor
t = time.time()
RF = RandomForestRegressor(max_depth=10, random_state=3, n_estimators=100)
RF.fit(x_train_tfv, y_train_tfv)
y_pred = RF.predict(x_test_tfv)
metric(y_test_tfv, y_pred)
print(time.time()-t)

"""Ужасные результаты, и время выполнения, забываем про этот метод"""

pip install catboost

from catboost import CatBoostRegressor

cat = CatBoostRegressor()
cat.fit(x_train_tfv, y_train_tfv, silent=True)
prediction = cat.predict(x_test_tfv)

metric(y_test_tfv, prediction)

"""Очень долго и при четверти датасета всего 0.35. Даже линейная справилась лучше. Хотя показатель RMSE здесь даже лучше, значит в среднем эта модель ближе

Глядя на графики и на метрики не однозначно, какой метод лучше, так как где-то графики примерно в одном диапазоне, но не совпадают. А другие (forest, catboost) имеют много точных совпадений, но частенько происходят резкие выбросы, скачки, которые портят итоговые метрики. Поэтому будем опираться на время и в данных реалиях выигрывают линейные методы.

## Нейросети
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model1 = Sequential()
model1.add(Dense(500, input_dim = x_train_tfv.shape[1], activation="relu"))
model1.add(Dense(190000, activation="softmax"))

model1.compile(loss='sparse_categorical_crossentropy', optimizer='SGD',metrics=["accuracy"])
model1.fit(x_train_tfv, y_train_tfv, epochs=10)

model1_loss, model1_acc = model1.evaluate(x_test_tfv, y_test_tfv)
print(f"Accuracy 1: {model1_loss}")
print(f"Loss 1: {mod1_loss}")

"""Объективно выводит очень плохую occuracy

## Вывод

**Умные выводы** 
Самым лучшим методов в плане соотношения время обучения, время реализации, точнойсть реализации по моему мнению является TfidfVectorizer в комбинации с линейными методами регрессии, хотя если есть время лучше возможно применить RandomForest или Catboost. Если смотреть со стороны быстроты, то нужно применить метод fasttext, хотя его обучение происходит долго, реализация происходит очень быстро.

**Впечатления** 
Объективно говоря, сама задумка этой лабораторной очень интересна. Вычисление ЗП по текстовым данным, разбираться было классно. Но все впечатление портили ооооооочень долгое ожидание и постоянные вылеты гугл коллаба, из-за чего приходилось снова и снова запускать код. И только к концу выполнения, я все же решился обрезать датасет, думаю нужно было прописать это в задании, так как считать одну модель 2 часа - это слишком. По выполнению можно сказать, какие методы лучше вычисляют то или иное значение. Конечно в итоге получилось маленькие показатели, их можно увеличить при увеличении мощностей, но даже так эта лаба и ее формат были интересны, и дали много знаний. Некоторые методы были упомянуты, но никак нельзя их было использовать, но благодаря тому, что в задании были они, несомненно было потрачено больше времени, но при этом и приобретено больше знаний.
"""